{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f910bcae",
   "metadata": {},
   "source": [
    "# Predicting Climate using Aerial Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46492018",
   "metadata": {},
   "source": [
    "## Creating Directories and Loading Images\n",
    "#### Here, we will be creating the directories required and loading the images. It is important to note that you must create an account with the link found here: https://scihub.copernicus.eu/dhus; this is to access the satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad085eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI\n",
    "\n",
    "user = '' # create an account following the link below and update the field here and below\n",
    "password = '' \n",
    "api = SentinelAPI(user, password, 'https://scihub.copernicus.eu/dhus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"\" # update this with your directory and your folder name for this project. I recommend using PredictingClimate as a folder name. \n",
    "pathTest = \"\" # update this with the above directory\n",
    "tempTest = \"\" # update this with a temporary directory\n",
    "try:\n",
    "    os.mkdir(pathTest)\n",
    "    os.mkdir(tempTest)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % pathTest)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % pathTest)\n",
    "\n",
    "latitude_lower_bound = 30\n",
    "latitude_upper_bound = 50\n",
    "longitude_lower_bound = -70\n",
    "longitude_upper_bound = -120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import shutil\n",
    "for latitude in range(latitude_lower_bound, latitude_upper_bound):\n",
    "    for longitude in range(longitude_upper_bound, longitude_lower_bound):\n",
    "        products = api.query('POINT({0} {1})'.format(longitude, latitude), platformname = 'Sentinel-2', processinglevel = 'Level-2A',cloudcoverpercentage = (0, 10))\n",
    "        products_gdf = api.to_geodataframe(products)\n",
    "        if (not(products_gdf.empty)):\n",
    "            print(\"Product found\")\n",
    "            products_gdf_sorted = products_gdf.sort_values(['cloudcoverpercentage'], ascending=[True])\n",
    "            name = products_gdf_sorted.iloc[0,:].name\n",
    "            api.download_quicklook(name, path)\n",
    "            \n",
    "            for count, filename in enumerate(os.listdir(\"\")): # update \"\" with your above created directory location\n",
    "                dst = \"l\" + str(longitude)+ \"l\" + str(latitude) + \".jpeg\"\n",
    "                src = '' + filename # update '' with your above created (not temporary) directory location\n",
    "                dst = '' + dst # update '' with your above created (not temporary) directory location\n",
    "                os.rename(src, dst)\n",
    "            \n",
    "            os.chdir('') # update this with your above created temporary directory location\n",
    "            dst_dir = \"\" # update this with your above created (not temporary directory location)\n",
    "            for f in os.listdir():\n",
    "                shutil.copy(f, dst_dir)\n",
    "            for file in os.listdir(''): # update this with your above created temporary directory location\n",
    "                if file.endswith('.jpeg'):\n",
    "                    os.remove(file)\n",
    "            time.sleep(1)\n",
    "                \n",
    "        print((longitude, latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0220d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "climateTestDir = \"\" # update this with a ClimateTest directory folder name\n",
    "climateTrainDir = \"\" # update this with a ClimateTrain directory folder name\n",
    "climates = [\"Af\", \"Am\", \"Aw\", \"As\", \n",
    "            \"BWh\", \"BWk\", \"BSh\", \"BSk\", \n",
    "            \"Csa\", \"Csb\", \"Csc\", \"Cwa\", \"Cwb\", \"Cwc\", \"Cfa\", \"Cfb\", \"Cfc\",\n",
    "            \"Dsa\", \"Dsb\", \"Dsc\", \"Dsd\", \"Dwa\", \"Dwb\", \"Dwc\", \"Dwd\", \"Dfa\", \"Dfb\", \"Dfc\", \"Dfd\",\n",
    "            \"ET\", \"EF\"]\n",
    "\n",
    "try:\n",
    "    os.mkdir(climateTestDir)\n",
    "    os.mkdir(climateTrainDir)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % climates)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % climates)\n",
    "    \n",
    "for i in range(len(climates)):\n",
    "    os.mkdir(climateTestDir + climates[i])\n",
    "    os.mkdir(climateTrainDir + climates[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fbcb8",
   "metadata": {},
   "source": [
    "## Sorting Images by Climate Type\n",
    "#### In these following coding blocks, we will be sorting the images according to their climate type. It is important to note that in this project, we are using the latitude and longitude values of the US. Simply adjust this to your liking, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "latitude_lower_bound = 30\n",
    "latitude_upper_bound = 50\n",
    "longitude_lower_bound = -120\n",
    "longitude_upper_bound = -70\n",
    "\n",
    "directory = \"\" # update this with the directory location above (not the temporary one)\n",
    "path = \".../data/koppen_1901-2010.tsv\" # update the ... with your directory location. Follow the download instructions at the website: http://hanschen.org/koppen\n",
    "koppen = np.genfromtxt(path, dtype=None, names=True)\n",
    "\n",
    "imageToClimate = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpeg\"):\n",
    "        newFile = filename.replace(\".jpeg\", '')\n",
    "        latLon = newFile.split(\"l\")\n",
    "        lat = (np.pi / 180) * int(latLon[2])\n",
    "        lon = (np.pi / 180) * int(latLon[1])\n",
    "\n",
    "        dist = [] \n",
    "        clim = []\n",
    "        \n",
    "        for i in range(len(koppen)):\n",
    "            if (koppen['latitude'][i] >= latitude_lower_bound and koppen['latitude'][i] <= latitude_upper_bound and koppen['longitude'][i] >= longitude_lower_bound and koppen['longitude'][i] <= longitude_upper_bound):\n",
    "                koppenLat = (np.pi / 180) * koppen['latitude'][i]\n",
    "                koppenLon = (np.pi / 180) * koppen['longitude'][i]\n",
    "                distance = math.sin(pow(((koppenLat-lat)/2), 2))+math.cos(koppenLat)*math.cos(lat)*math.sin(pow(((koppenLon-lon)/2), 2))\n",
    "                dist.append(distance)\n",
    "                clim.append(koppen['p1901_2010'][i])\n",
    "        \n",
    "        closestClimateType = clim[dist.index(min(dist))]\n",
    "        imageToClimate[filename] = closestClimateType\n",
    "    \n",
    "pickle.dump(imageToClimate, open(\"imageToClimate.p\", \"wb\"))\n",
    "imageToClimate = pickle.load(open(\"imageToClimate.p\", \"rb\"))\n",
    "print(imageToClimate)\n",
    "\n",
    "dst = \"\" # update this with your temporary directory location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbdbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imageToClimate:\n",
    "    print(i)\n",
    "    origClimate = str(imageToClimate.get(i))\n",
    "    newClim = str(origClimate[2:-1])\n",
    "    moveToClimate = dst + newClim\n",
    "    source = directory + \"/\" + i\n",
    "    shutil.copy(source, moveToClimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, shutil, math\n",
    "path = \"\" # update this with the ClimateTest directory location\n",
    "newPath = \"\" # update this with the CliamteTrain directory location\n",
    "\n",
    "counter = 0\n",
    "origList = []\n",
    "climList = []\n",
    "\n",
    "for i in os.listdir(path):\n",
    "    for j in os.listdir(path + \"/\" + i):\n",
    "        counter = counter + 1\n",
    "        origList.append(path + \"/\" + i + \"/\" + j)\n",
    "        climList.append(newPath + \"/\" + i)\n",
    "    \n",
    "\n",
    "bigList = list(zip(origList, climList))\n",
    "random.shuffle(bigList)\n",
    "\n",
    "origList, climList = zip(*bigList)\n",
    "\n",
    "dataTrainAmount = math.ceil(counter * 0.8)\n",
    "dataTestAmount = math.ceil(counter * 0.2)\n",
    "\n",
    "for i in range(dataTrainAmount):\n",
    "    shutil.move(origList[i], climList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c10149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, shutil, math\n",
    "path = \"\" # update this with the ClimateTrain directory location\n",
    "newPath = \"\" # update this with a ClimateVal directory location\n",
    "\n",
    "# uncomment this code to create the ClimateVal directory\n",
    "#try:\n",
    "    #os.mkdir(newPath)\n",
    "#except OSError:\n",
    "#    print (\"Creation of the directory %s failed\" % newPath)\n",
    "#else:\n",
    "#    print (\"Successfully created the directory %s \" % newPath)\n",
    "\n",
    "climates = [\"Af\", \"Am\", \"Aw\", \"As\", \n",
    "            \"BWh\", \"BWk\", \"BSh\", \"BSk\", \n",
    "            \"Csa\", \"Csb\", \"Csc\", \"Cwa\", \"Cwb\", \"Cwc\", \"Cfa\", \"Cfb\", \"Cfc\",\n",
    "            \"Dsa\", \"Dsb\", \"Dsc\", \"Dsd\", \"Dwa\", \"Dwb\", \"Dwc\", \"Dwd\", \"Dfa\", \"Dfb\", \"Dfc\", \"Dfd\",\n",
    "            \"ET\", \"EF\"]\n",
    "\n",
    "# uncomment this to fill the climates\n",
    "#for i in range(len(climates)):\n",
    "    #print(climates[i])\n",
    "    #os.mkdir(newPath + climates[i])\n",
    "    \n",
    "counter = 0\n",
    "origList = []\n",
    "climList = []\n",
    "\n",
    "for i in os.listdir(path):\n",
    "    for j in os.listdir(path + \"/\" + i):\n",
    "        counter = counter + 1\n",
    "        origList.append(path + \"/\" + i + \"/\" + j)\n",
    "        climList.append(newPath + \"/\" + i)\n",
    "    \n",
    "bigList = list(zip(origList, climList))\n",
    "random.shuffle(bigList)\n",
    "\n",
    "origList, climList = zip(*bigList)\n",
    "\n",
    "dataValAmount = math.ceil(counter * 0.1)\n",
    "\n",
    "for i in range(dataValAmount):\n",
    "    shutil.move(origList[i], climList[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f52af6",
   "metadata": {},
   "source": [
    "## CNN Training using Tensorflow\n",
    "#### We can now use the above code blocks to train our model. We will be using a tensorflow CNN model, which will be modified to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = \"\" # update this with Climates directory\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ee54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de577b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cea0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec353da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomFlip(\"vertical\",\n",
    "                     input_shape=(img_height,\n",
    "                                 img_width,\n",
    "                                 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c02d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "path = \"\" # update this with Cliamtes directory location\n",
    "\n",
    "y_actu = []\n",
    "y_pred = []\n",
    "\n",
    "arr = os.listdir(path)\n",
    "\n",
    "for i in arr:\n",
    "    newPath = path + \"/\" + i\n",
    "    for j in os.listdir(newPath):\n",
    "        cfa_image = newPath + \"/\" + j\n",
    "        img = tf.keras.utils.load_img(\n",
    "            cfa_image, target_size=(img_height, img_width)\n",
    "            )\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        print(i)\n",
    "        y_actu.append(i)\n",
    "        y_pred.append(class_names[np.argmax(score)])\n",
    "        \n",
    "confArr = confusion_matrix(y_actu, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "def plot_confusion_matrix(df_confusion, cmap=plt.cm.plasma):\n",
    "    plt.matshow(df_confusion, cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "row_sums = confArr.sum(axis=1)\n",
    "new_matrix = confArr / row_sums[:, numpy.newaxis]\n",
    "plot_confusion_matrix(new_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
